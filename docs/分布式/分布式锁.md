## 为什么需要分布式锁

与分布式锁相对应的是「单机锁」，我们在写多线程程序时，避免同时操作一个共享变量产生数据问题，通常会使用一把锁来「互斥」，以保证共享变量的正确性，其使用范围是在「同一个进程」中。

如果换做是多个进程，需要同时操作一个共享资源，如何互斥呢？

例如，现在的业务应用通常都是微服务架构，这也意味着一个应用会部署多个进程，那这多个进程如果需要修改 MySQL 中的同一行记录时，为了避免操作乱序导致数据错误，此时，我们就需要引入「分布式锁」来解决这个问题了。

![](../../images/20220227-1.jpg)

想要实现分布式锁，必须借助一个外部系统，所有进程都去这个系统上申请「加锁」。

而这个外部系统，必须要实现「互斥」的能力，即两个请求同时进来，只会给一个进程返回成功，另一个返回失败（或等待）。

这个外部系统，可以是 MySQL，也可以是 Redis 或 Zookeeper。但为了追求更好的性能，我们通常会选择使用 Redis 或 Zookeeper 来做。

### 一个好的分布式锁应该具有的特性

* 可重入
* 同一个时间点，只有一个线程持有锁
* 容错性，当锁节点宕机时，能及时释放锁
* 高性能
* 无单点问题

## 实现方式

整体流程：

![](../../images/20220227-2.png)

### MySQL

基于数据库的分布式锁, 常用的一种方式是使用表的唯一约束特性。当往数据库中成功插入一条数据时, 代表只获取到锁。将这条数据从数据库中删除，则释放锁。

我们需要创建一张表：

```sql
CREATE TABLE `methodLock` (
  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '主键',
  `method_name` varchar(64) NOT NULL DEFAULT '' COMMENT '锁定的方法名',
  `cust_id` varchar(1024) NOT NULL DEFAULT '客户端唯一编码',
  `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '保存数据时间，自动生成',
  PRIMARY KEY (`id`),
  UNIQUE KEY `uidx_method_name` (`method_name `) USING BTREE
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
```

添加锁：

```sql
insert into methodLock(method_name,cust_id) values (?, ?);
```

这里 cust_id 可以是机器的 mac 地址 + 线程编号, 确保一个线程只有唯一的一个编号。通过这个编号， 可以有效的判断是否为锁的创建者，从而进行锁的释放以及重入锁判断。

释放锁：

```sql
delete from methodLock where method_name = ? and cust_id = ?;
```

重入锁判断:

```sql
select 1 from methodLock where method_name = ? and cust_id = ?;
```

#### 优点

操作简单，容易理解。

#### 缺点

性能开销大。

### Redis

下面采用层层递进的方式，一步步优化 Redis 分布式锁的实现方式，解决各种各样的缺陷。

#### 最基础的实现方式

想要实现分布式锁，必须要求 Redis 有「互斥」的能力，我们可以使用 SETNX 命令，这个命令表示SET if Not eXists，即如果 key 不存在，才会设置它的值，否则什么也不做。

两个客户端进程可以执行这个命令，达到互斥，就可以实现一个分布式锁。

客户端 1 申请加锁，加锁成功：

```bash
127.0.0.1:6379> SETNX lock 1
(integer) 1     // 客户端1，加锁成功
```

客户端 2 申请加锁，因为后到达，加锁失败：

```bash
127.0.0.1:6379> DEL lock // 释放锁
(integer) 1
```

此时，加锁成功的客户端，就可以去操作「共享资源」，例如，修改 MySQL 的某一行数据，或者调用一个 API 请求。

操作完成后，还要及时释放锁，给后来者让出操作共享资源的机会。如何释放锁呢？

也很简单，直接使用 DEL 命令删除这个 key 即可：

```bash
127.0.0.1:6379> DEL lock // 释放锁
(integer) 1
```

这个逻辑非常简单，整体的路程就是这样：

![](../../images/20220228-1.jpg)

但是，它存在一个很大的问题，当客户端 1 拿到锁后，如果发生下面的场景，就会造成「死锁」：

1. 程序处理业务逻辑异常，没及时释放锁
2. 进程挂了，没机会释放锁

这时，这个客户端就会一直占用这个锁，而其它客户端就「永远」拿不到这把锁了。

#### 使用 EX 解决「死锁」问题

在 Redis 2.6.12 版本之前，我们需要想尽办法，保证 SETNX 和 EXPIRE 原子性执行，还要考虑各种异常情况如何处理。

但在 Redis 2.6.12 之后，Redis 扩展了 SET 命令的参数，用这一条命令就可以了：

```bash
// 一条命令保证原子性执行
127.0.0.1:6379> SET lock 1 EX 10 NX
OK
```

这样就解决了死锁问题，也比较简单。

我们再来看分析下，它还有什么问题？

试想这样一种场景：

1. 客户端 1 加锁成功，开始操作共享资源
2. 客户端 1 操作共享资源的时间，「超过」了锁的过期时间，锁被「自动释放」
3. 客户端 2 加锁成功，开始操作共享资源
4. 客户端 1 操作共享资源完成，释放锁（但释放的是客户端 2 的锁）

这里存在两个严重的问题：
1. <strong>锁过期</strong>：客户端 1 操作共享资源耗时太久，导致锁被自动释放，之后被客户端 2 持有
2. <strong>释放别人的锁</strong>：客户端 1 操作共享资源完成后，却又释放了客户端 2 的锁

对于锁过期，我们并不能给定一个固定的过期时间，覆盖到所有的可能出现的情况，所以我们需要一个新的方法来解决这个问题。而对于释放别人的锁，我们可以加锁时使用 UUID 来解决。

#### 使用 UUID 解决释放他人锁的问题

解决办法是：客户端在加锁时，设置一个只有自己知道的「唯一标识」进去。

例如，可以是自己的线程 ID，也可以是一个 UUID（随机且唯一），这里我们以 UUID 举例：

```bash
// 锁的VALUE设置为UUID
127.0.0.1:6379> SET lock $uuid EX 20 NX
OK
```

这里假设 20s 操作共享时间完全足够，先不考虑锁自动过期的问题。

之后，在释放锁时，要先判断这把锁是否还归自己持有，伪代码可以这么写：

```
// 锁是自己的，才释放
if redis.get("lock") == $uuid:
    redis.del("lock")
```

判断 UUID 和删除锁的操作必须保证原子执行才行。

1. 客户端 1 执行 GET，判断锁是自己的
2. 客户端 2 执行了 SET 命令，强制获取到锁（虽然发生概率比较低，但我们需要严谨地考虑锁的安全性模型）
3. 客户端 1 执行 DEL，却释放了客户端 2 的锁

由此可见，这两个命令还是必须要原子执行才行。

我们可以把这个逻辑，写成 Lua 脚本，让 Redis 来执行。

因为 Redis 处理每一个请求是「单线程」执行的，在执行一个 Lua 脚本时，其它请求必须等待，直到这个 Lua 脚本处理完成，这样一来，GET + DEL 之间就不会插入其它命令了。

安全释放锁的 Lua 脚本如下：

```bash
// 判断锁是自己的，才释放
if redis.call("GET",KEYS[1]) == ARGV[1]
then
    return redis.call("DEL",KEYS[1])
else
    return 0
end
```

现在这个分布式锁已经越来越严谨了。

这里我们先小结一下，基于 Redis 实现的分布式锁，一个严谨的的流程如下：

1. 加锁：SET $lock_key $unique_id EX $expire_time NX
2. 操作共享资源
3. 释放锁：Lua 脚本，先 GET 判断锁是否归属自己，再 DEL 释放锁

![](../../images/20220228-2.jpg)

好，有了这个完整的锁模型，让我们重新回到前面提到的第一个问题。

锁过期时间不好评估怎么办？

#### 使用守护线程解决固定过期时间的问题

加锁时，先设置一个过期时间，然后我们开启一个「守护线程」，定时去检测这个锁的失效时间，如果锁快要过期了，操作共享资源还未完成，那么就自动对锁进行「续期」，重新设置过期时间。

如果你是 Java 技术栈，幸运的是，已经有一个库把这些工作都封装好了：Redisson。

Redisson 是一个 Java 语言实现的 Redis SDK 客户端，在使用分布式锁时，它就采用了「自动续期」的方案来避免锁过期，这个守护线程我们一般也把它叫做「看门狗」线程。

这个 SDK 提供的 API 非常友好，它可以像操作本地锁的方式，操作分布式锁。如果你是 Java 技术栈，可以直接把它用起来。

上面分析的 Redis 实现的分布式锁都没考虑到多实例部署的情况，而我们在使用 Redis 时，一般会采用主从集群 + 哨兵的模式部署，这样做的好处在于，当主库异常宕机时，哨兵可以实现「故障自动切换」，把从库提升为主库，继续提供服务，以此保证可用性。

那当「主从发生切换」时，这个分布锁会依旧安全吗？

试想这样的场景：

1. 客户端 1 在主库上执行 SET 命令，加锁成功
2. 此时，主库异常宕机，SET 命令还未同步到从库上（主从复制是异步的）
3. 从库被哨兵提升为新主库，这个锁在新的主库上，丢失了！

可见，当引入 Redis 副本后，分布锁还是可能会受到影响。

#### 使用 Redlock 解决主从复制锁丢失的问题

Redlock 的方案基于 2 个前提：

1. 不再需要部署从库和哨兵实例，只部署主库
2. 但主库要部署多个，官方推荐至少 5 个实例

也就是说，想用使用 Redlock，你至少要部署 5 个 Redis 实例，而且都是主库，它们之间没有任何关系，都是一个个孤立的实例。

![](../../images/20220323-1.jpg)

Redlock 整体使用流程：

1. 客户端先获取「当前时间戳T1」
2. 客户端依次向这 5 个 Redis 实例发起加锁请求（用前面讲到的 SET 命令），且每个请求会设置超时时间（毫秒级，要远小于锁的有效时间），如果某一个实例加锁失败（包括网络超时、锁被其它人持有等各种异常情况），就立即向下一个 Redis 实例申请加锁
3. 如果客户端从 >=3 个（大多数）以上 Redis 实例加锁成功，则再次获取「当前时间戳T2」，如果 T2 - T1 < 锁的过期时间，此时，认为客户端加锁成功，否则认为加锁失败
4. 加锁成功，去操作共享资源（例如修改 MySQL 某一行，或发起一个 API 请求）
5. 加锁失败，向「全部节点」发起释放锁请求（前面讲到的 Lua 脚本释放锁）

有 4 个重点：

1. 客户端在多个 Redis 实例上申请加锁
2. 必须保证大多数节点加锁成功
3. 大多数节点加锁的总耗时，要小于锁设置的过期时间
4. 释放锁，要向全部节点发起释放锁请求

#### 优点

redis 的性能很高，可以支撑高并发的获取、释放锁操作。

#### 缺点

获取锁的方式简单粗暴，获取不到锁直接不断尝试获取锁，比较消耗性能。

即便使用 redlock 算法来实现，在极端场景下，也无法保证其实现 100% 没有问题。

##### 理论上的极端场景

<strong>GC：</strong>

* 客户端 1 请求锁定节点 A、B、C、D、E
* 客户端 1 的拿到锁后，进入 GC
* 所有 Redis 节点上的锁都过期了
* 客户端 2 获取节点 A、B、C、D、E 上的锁
* 客户端 1 GC 结束，认为成功获取锁
* 客户端 2 也认为获取到锁，发生「冲突」

如果是在第 1 - 3 步 GC，redlock 通过 T2 - T1，是可以检测出来的，如果超出了锁设置的过期时间，那这时就认为加锁会失败，之后释放所有节点的锁就好了（T2 - T1 所剩时间不多的话会自动续期，保证共享资源的操作执行完成）。但是如果在第 3 步之后 GC，也就是客户端确认拿到了锁，去操作共享资源的途中发生了问题，导致锁失效，此时有其他线程可以获取到锁，就可能发生「冲突」（Zookeeper 也会有这个问题）。

<strong>时钟漂移：</strong>

* 客户端 1 获取节点 A、B、C 上的锁，但由于网络问题，无法访问 D 和 E
* 节点 C 上的时钟「向前跳跃」，导致锁到期
* 客户端 2 获取节点 C、D、E 上的锁，由于网络问题，无法访问 A 和 B
* 客户端 1 和 2 现在都相信它们持有了锁（冲突）

### Zookeeper

zk 的模型是这样的：zk 包含一系列的节点，叫做 znode，就好像文件系统一样每个 znode 表示一个目录，然后znode 有一些特性：

有序节点：假如当前有一个父节点为 /lock，我们可以在这个父节点下面创建子节点。zookeeper 提供了一个可选的有序特性，例如我们可以创建子节点 /lock/node- 并且指明有序，那么 zookeeper 在生成子节点时会根据当前的子节点数量自动添加整数序号。也就是说，如果是第一个创建的子节点，那么生成的子节点为 /lock/node-0000000000，下一个节点则为 /lock/node-0000000001，依次类推。  

临时节点：客户端可以建立一个临时节点，在会话结束或者会话超时（相当于 redis 给锁设置超时时间）后，zookeeper 会自动删除该节点。

事件监听：在读取数据时，我们可以同时对节点设置事件监听，当节点数据或结构变化时，zookeeper 会通知客户端。当前 zookeeper 有如下四种事件：节点创建，节点删除，节点数据修改，子节点变更。

基于以上的一些 zk 的特性，我们很容易得出使用 zk 实现分布式锁的落地方案：

1. 使用 zk 的临时节点和有序节点，每个线程获取锁就是在zk创建一个临时有序的节点，比如在 /lock/ 目录下。
2. 创建节点成功后，获取 /lock 目录下的所有临时节点，再判断当前线程创建的节点是否是所有的节点的序号最小的节点。
3. 如果当前线程创建的节点是所有节点序号最小的节点，则认为获取锁成功。如果当前线程创建的节点不是所有节点序号最小的节点，则对节点序号的前一个节点添加一个事件监听。比如当前线程获取到的节点序号为 /lock/003，然后所有的节点列表为[/lock/001,/lock/002,/lock/003]，则对 /lock/002 这个节点添加一个事件监听器。
4. 如果锁释放了，会唤醒下一个序号的节点，然后重新执行第 3 步，判断是否自己的节点序号是最小。比如 /lock/001 释放了，/lock/002 监听到事件，此时节点集合为[/lock/002,/lock/003],则 /lock/002 为最小序号节点，获取到锁。

#### 优点

zookeeper 天生设计定位就是分布式协调，强一致性。锁的模型健壮、简单易用、适合做分布式锁。

如果获取不到锁，只需要添加一个监听器就可以了，不用一直轮询，性能消耗较小。

#### 缺点

性能不如 Redis，如果有较多的客户端频繁的申请加锁、释放锁，对于 zk 集群的压力会比较大。

## 思考

### 分布式锁和乐观锁的比较

### Redis 分布式锁守护线程相比于不设置过期时间的意义

如果服务宕机了，Watch Dog 机制线程也就没有了，此时就不会延长 key 的过期时间，到了 30s 之后就会自动过期了，其他线程就可以获取到锁。

如果不设置过期时间，让业务运行结束后解锁，但是如果客户端出现了异常结束了或宕机了，那么这个锁就无法解锁，变成死锁。

### Redis 分布式锁守护线程的原理

Watch Dog 机制其实就是一个后台定时任务线程，获取锁成功之后，会将持有锁的线程放入到一个 RedissonLock.EXPIRATION_RENEWAL_MAP 里面，然后每隔 10 秒 （internalLockLeaseTime / 3） 检查一下，如果客户端 还持有锁 key（判断客户端是否还持有 key，其实就是遍历 EXPIRATION_RENEWAL_MAP 里面线程 id 然后根据线程 id 去 Redis 中查，如果存在就会延长 key 的时间），那么就会不断的延长锁 key 的生存时间。

### 如何更好的使用分布式锁

使用分布式锁，在上层完成「互斥」目的，虽然极端情况下锁会失效，但它可以最大程度把并发请求阻挡在最上层，减轻操作资源层的压力。

但对于要求数据绝对正确的业务，在资源层一定要做好「兜底」，设计思路可以借鉴 fencing token 的方案来做（类似乐观锁）。

<strong>主要思想还是只使用分布式锁做资源请求的过滤，减少并发的概率，要保证觉得的数据业务正确，需要有自己的兜底方案。</strong>


